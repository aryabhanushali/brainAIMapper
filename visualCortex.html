<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Visual Cortex - Brain-AI Mapper</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0/dist/tf.min.js"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 2rem;
      background: radial-gradient(ellipse at center, #0d1117 0%, #0a0d14 100%);
      color: #f8fafc;
    }
    .card {
      max-width: 960px;
      margin: auto;
      background: rgba(255, 255, 255, 0.05);
      border-radius: 20px;
      padding: 2.5rem;
      border: 1px solid rgba(255, 255, 255, 0.08);
    }
    h2, h3 {
      color: #60a5fa;
      text-align: center;
      margin-top: 1rem;
      font-weight: 600;
      letter-spacing: 0.01em;
      text-shadow: 0 0 8px #60a5fa33, 0 0 2px #f8fafc44;
    }
    .section {
      max-width: 720px;
      margin: 2rem auto;
      font-size: 1rem;
      line-height: 1.6;
      color: #cbd5e1;
    }
    .side-by-side {
      display: flex;
      gap: 32px;
      justify-content: center;
      align-items: flex-start;
      flex-wrap: wrap;
      margin: 2rem 0;
    }
    .brain-box, .ai-box {
      flex: 1 1 320px;
      background: rgba(99,179,237,0.08);
      border-radius: 14px;
      padding: 1.2rem 1.5rem;
      min-width: 260px;
      box-shadow: 0 2px 12px rgba(99,179,237,0.07);
    }
    .brain-box {
      border-left: 4px solid #66f9da;
    }
    .ai-box {
      border-left: 4px solid #a78bfa;
    }
    .step {
      margin-bottom: 1.1rem;
      display: flex;
      align-items: flex-start;
      gap: 0.7em;
    }
    .step-emoji {
      font-size: 1.5em;
      margin-top: 2px;
    }
    .compare-table {
      width: 100%;
      border-collapse: separate;
      border-spacing: 0;
      color: #f1f5f9;
      margin: 2rem 0 1rem 0;
      background: rgba(30,41,59,0.85);
      border-radius: 16px;
      overflow: hidden;
      font-size: 1.04rem;
    }
    th, td {
      padding: 18px 18px;
      vertical-align: top;
      text-align: left;
    }
    th {
      background: #1e293b;
      color: #60a5fa;
      font-weight: 600;
      letter-spacing: 0.01em;
      text-shadow: 0 0 6px #60a5fa33;
    }
    td {
      background: rgba(30,41,59,0.72);
      border-bottom: 1px solid #334155;
      border-top: none;
      border-left: none;
      border-right: none;
      transition: background 0.2s;
    }
    .compare-table tr:last-child td {
      border-bottom: none;
    }
    .compare-table tr:hover td {
      background: #2d3650;
    }
    .compare-table th:first-child {
      border-top-left-radius: 16px;
    }
    .compare-table th:last-child {
      border-top-right-radius: 16px;
    }
    .compare-table tr:last-child td:first-child {
      border-bottom-left-radius: 16px;
    }
    .compare-table tr:last-child td:last-child {
      border-bottom-right-radius: 16px;
    }
    .image-section {
      display: flex;
      justify-content: center;
      gap: 30px;
      flex-wrap: wrap;
      margin: 1.5rem 0;
    }
    canvas, img {
      border-radius: 12px;
      border: 1px solid #334155;
      background: #0a0d14;
    }
    input, select {
      background: #0f172a;
      color: #f8fafc;
      border: 1px solid #334155;
      padding: 8px 12px;
      border-radius: 8px;
    }
    .filter-section {
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    .back-link {
      text-decoration: none;
      color: #94a3b8;
      display: inline-block;
      margin-bottom: 1rem;
      font-size: 0.95rem;
    }
    .back-link:hover {
      color:  #eeeeee;
      ;
    }
    .caption-box {
      background: #1e293b;
      padding: 1rem;
      border-radius: 10px;
      text-align: center;
      margin-top: 1rem;
    }
    .demo-label {
      text-align: center;
      color: #60a5fa;
      font-weight: 600;
      font-size: 1.1rem;
      margin-bottom: 0.5rem;
      margin-top: 2.2rem;
      letter-spacing: 0.02em;
      text-shadow: 0 0 8px #60a5fa33;
    }
    pre {
      font-family: monospace;
    }
    #flow-diagram-bg {
      filter: blur(8px);
    }
    .flow-label {
      font-size: 1.08rem;
      fill: #cbd5e1;
      font-weight: 600;
      letter-spacing: 0.01em;
    }
    .flow-node {
      filter: drop-shadow(0 2px 12px #0005);
      cursor: pointer;
      transition: filter 0.2s;
    }
    .flow-node:hover {
      filter: drop-shadow(0 4px 24px #fff8);
    }
    .flow-arrow {
      stroke-dasharray: 8 6;
      animation: dashmove 2s linear infinite;
    }
    @keyframes dashmove {
      to { stroke-dashoffset: -28; }
    }
    .glossary-term {
      border-bottom: 1px dotted #60a5fa;
      cursor: help;
      color: #60a5fa;
      position: relative;
      transition: color 0.2s;
    }
    .glossary-term:hover, .glossary-term:focus {
      color: #38bdf8;
    }
    .glossary-tooltip {
      position: fixed;
      z-index: 9999;
      background: #1e293b;
      color: #f1f5f9;
      padding: 0.7em 1em;
      border-radius: 10px;
      font-size: 1rem;
      box-shadow: 0 4px 24px #0008;
      pointer-events: none;
      opacity: 0;
      transition: opacity 0.18s, transform 0.18s;
      max-width: 320px;
      line-height: 1.5;
      white-space: normal;
    }
    .glossary-tooltip.visible {
      opacity: 1;
      transform: translateY(-8px);
    }
    /* Section fade/slide-in animation */
    .fade-section {
      opacity: 0;
      transform: translateY(32px);
      transition: opacity 0.7s cubic-bezier(.4,0,.2,1), transform 0.7s cubic-bezier(.4,0,.2,1);
    }
    .fade-section.visible {
      opacity: 1;
      transform: none;
    }
    h2.fade-section, h3.fade-section {
      transition-delay: 0.1s;
    }
  </style>
</head>
<body>

<div class="card">
  <a class="back-link" href="index.html">← Back to Brain Map</a>

  <h2 class="fade-section" style="color:#60a5fa; text-shadow:0 0 8px #60a5fa33, 0 0 2px #f8fafc44;">1. How Humans Process Images with the Visual Cortex</h2>
  <div class="section fade-section">
    <ol>
      <li><b>Retina:</b> Light hits the eye and is converted to electrical signals.</li>
      <li><b><span class="glossary-term" data-term="V1">V1 (Primary Visual Cortex)</span>:</b> Detects edges, lines, and simple patterns.</li>
      <li><b><span class="glossary-term" data-term="V2">V2–V4</span>:</b> Combines edges into shapes, colors, and textures.</li>
      <li><b><span class="glossary-term" data-term="IT cortex">IT Cortex</span>:</b> Recognizes whole objects and faces using memory and experience.</li>
    </ol>
  </div>

  <h2 class="fade-section" style="color:#60a5fa; text-shadow:0 0 8px #60a5fa33, 0 0 2px #f8fafc44;">2. How AI is Inspired by the Visual Cortex</h2>
  <div class="section fade-section">
    <ol>
      <li><b>Input:</b> Image is fed into the neural network as pixel data.</li>
      <li><b><span class="glossary-term" data-term="convolution">Convolution</span>:</b> Filters scan for edges and patterns (like <span class="glossary-term" data-term="V1">V1</span>).</li>
      <li><b><span class="glossary-term" data-term="activation">Activation</span>:</b> Non-linear function highlights important features.</li>
      <li><b><span class="glossary-term" data-term="pooling">Pooling</span>:</b> Reduces detail, keeps key info (like <span class="glossary-term" data-term="V2">V2–V4</span>).</li>
      <li><b>Fully Connected:</b> Combines features to recognize objects (like <span class="glossary-term" data-term="IT cortex">IT Cortex</span>).</li>
    </ol>
  </div>

  <h2 class="fade-section" style="color:#60a5fa; text-shadow:0 0 8px #60a5fa33, 0 0 2px #f8fafc44;">3. Human Visual Cortex vs. CNN</h2>
  <table class="compare-table fade-section" style="margin-bottom: 2.5rem;">
    <colgroup>
      <col style="width: 50%;">
      <col style="width: 50%;">
    </colgroup>
    <tr>
      <th>Human Visual Cortex</th>
      <th>AI (<span class="glossary-term" data-term="convolutional neural network">Convolutional Neural Network</span>)</th>
    </tr>
    <tr>
      <td>Retina captures light, sends signals to brain</td>
      <td>Image pixels are input to the network</td>
    </tr>
    <tr>
      <td><span class="glossary-term" data-term="V1">V1</span> detects edges, lines, and orientations</td>
      <td>First <span class="glossary-term" data-term="convolution">convolutional</span> layer finds edges and simple patterns</td>
    </tr>
    <tr>
      <td><span class="glossary-term" data-term="V2">V2–V4</span> combine features into shapes, colors, textures</td>
      <td>Deeper layers combine features into complex patterns</td>
    </tr>
    <tr>
      <td><span class="glossary-term" data-term="IT cortex">IT cortex</span> recognizes objects and faces using memory</td>
      <td>Final layers recognize objects, faces, or scenes</td>
    </tr>
  </table>

  <div class="demo-label fade-section">Try it: See how AI "sees" like your brain!</div>
  <div class="section fade-section" style="text-align:center;">
    Upload an image and choose a filter. The AI will process the image using filters similar to those in your visual cortex:
    <ul style="margin: 0.5rem auto 0; max-width: 400px; text-align: left;">
      <li><b><span class="glossary-term" data-term="sobel filter">Sobel</span>:</b> Detects edges (like <span class="glossary-term" data-term="V1">V1</span> neurons)</li>
      <li><b><span class="glossary-term" data-term="sharpen filter">Sharpen</span>:</b> Highlights borders (focuses attention)</li>
      <li><b><span class="glossary-term" data-term="blur filter">Blur</span>:</b> Smooths details (like peripheral vision)</li>
    </ul>
  </div>

  <div class="filter-section fade-section">
    <input type="file" id="imageUpload" accept="image/*" />
    <label for="customFilter">Choose a filter:</label>
    <select id="customFilter">
      <option value="sobel">Sobel (Edge Detection)</option>
      <option value="sharpen">Sharpen</option>
      <option value="blur">Blur</option>
    </select>
    <p id="filter-explanation">Sobel filter detects edges by highlighting horizontal and vertical intensity changes.</p>
  </div>

  <div class="image-section fade-section">
    <img id="originalImage" width="256" height="256" />
    <canvas id="filterCanvas" width="256" height="256"></canvas>
  </div>

  <div class="section fade-section">
    <h3>How Filters Work</h3>
    <p>
      Filters in CNNs are small grids (<span class="glossary-term" data-term="kernel">kernels</span>) that scan across an image, performing a <span class="glossary-term" data-term="convolution">convolution</span>: they multiply pixel values within a small neighborhood and sum the results. This operation helps extract visual patterns:
    </p>
    <ul>
      <li><strong>Sobel Filter:</strong> Detects <em>vertical edges</em>. It simulates how neurons in the <span class="glossary-term" data-term="V1">V1 visual cortex</span> respond to orientation and contrast.</li>
      <li><strong>Sharpen Filter:</strong> Highlights borders by emphasizing differences between neighboring pixels. Mimics focused attention in vision.</li>
      <li><strong>Blur Filter:</strong> Averages pixel values in a local area, reducing detail and noise. Similar to how peripheral vision simplifies visual input.</li>
    </ul>
    <h3>Example Filter Kernels</h3>
    <table>
      <tr>
        <th>Filter</th>
        <th>Kernel Matrix</th>
        <th>Explanation</th>
      </tr>
      <tr>
        <td><span class="glossary-term" data-term="sobel filter">Sobel X</span></td>
        <td>
<pre>[
 [-1,  0,  1],
 [-2,  0,  2],
 [-1,  0,  1]
]</pre>
        </td>
        <td>
          Detects <strong>vertical edges</strong> by subtracting left from right intensity. Highlights where brightness changes horizontally—just like edge-sensitive neurons in your brain.
        </td>
      </tr>
      <tr>
        <td><span class="glossary-term" data-term="sharpen filter">Sharpen</span></td>
        <td>
<pre>[
 [ 0, -1,  0],
 [-1,  5, -1],
 [ 0, -1,  0]
]</pre>
        </td>
        <td>
          Enhances contrast by <strong>amplifying the center</strong> pixel and subtracting neighbors. Makes features like edges and texture "pop" — similar to focused attention.
        </td>
      </tr>
      <tr>
        <td><span class="glossary-term" data-term="blur filter">Blur</span></td>
        <td>
<pre>[
 [1, 1, 1],
 [1, 1, 1],
 [1, 1, 1]
] ÷ 9</pre>
        </td>
        <td>
          Averages all 9 surrounding pixels to <strong>smooth out detail</strong> and reduce noise. Mimics peripheral or unfocused visual processing in the human brain.
        </td>
      </tr>
    </table>
    <p><em>These kernels help CNNs mimic early visual processing, from simple contrasts to object outlines, just like the human brain.</em></p>
  </div>

  <div class="section fade-section" style="text-align:center; color:#60a5fa; font-weight:600; font-size:1.1rem; text-shadow:0 0 8px #60a5fa33, 0 0 2px #f8fafc44;">
    <b>Summary:</b> Both your brain and AI break down images into simple features, then build up to complex understanding. The next time you see a photo, remember: your brain is running its own neural network!
  </div>
</div>

<!-- Glossary tooltip element -->
<div id="glossary-tooltip" class="glossary-tooltip"></div>

<script>
const originalImage = document.getElementById('originalImage');
const filterCanvas = document.getElementById("filterCanvas");
const filterCtx = filterCanvas.getContext("2d");
const imageUpload = document.getElementById('imageUpload');
const filterSelect = document.getElementById("customFilter");
const filterExplanation = document.getElementById("filter-explanation");

const explanations = {
  sobel: "Sobel filter detects edges by highlighting horizontal and vertical intensity changes.",
  sharpen: "Sharpen filter enhances borders by amplifying contrast between neighboring pixels.",
  blur: "Blur filter smooths the image by averaging neighboring pixels, reducing sharp features."
};

imageUpload.addEventListener('change', (e) => {
  const file = e.target.files[0];
  if (!file) return;
  const reader = new FileReader();
  reader.onload = (event) => {
    originalImage.src = event.target.result;
  };
  reader.readAsDataURL(file);
});

originalImage.onload = () => {
  filterCtx.drawImage(originalImage, 0, 0, 256, 256);
  applyFilter(filterSelect.value);
};

filterSelect.addEventListener("change", () => {
  if (originalImage.src) applyFilter(filterSelect.value);
  filterExplanation.textContent = explanations[filterSelect.value];
});

async function applyFilter(type) {
  let imgTensor = tf.browser.fromPixels(filterCanvas)
    .mean(2).toFloat().expandDims(0).expandDims(-1);

  let kernel;
  if (type === "sobel") {
    kernel = tf.tensor4d([[[[-1]], [[0]], [[1]]], [[[-2]], [[0]], [[2]]], [[[-1]], [[0]], [[1]]]], [3, 3, 1, 1]);
  } else if (type === "sharpen") {
    kernel = tf.tensor4d([[[[0]], [[-1]], [[0]]], [[[-1]], [[5]], [[-1]]], [[[0]], [[-1]], [[0]]]], [3, 3, 1, 1]);
  } else if (type === "blur") {
    kernel = tf.tensor4d([[[[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]]], [[[1]], [[1]], [[1]]]], [3, 3, 1, 1]).div(9);
  }

  const result = tf.conv2d(imgTensor, kernel, 1, 'same').squeeze();
  const normalized = result.sub(result.min()).div(result.max().sub(result.min())).mul(255).toInt();
  await tf.browser.toPixels(normalized, filterCanvas);

  tf.dispose([imgTensor, kernel, result, normalized]);
}
</script>

<script>
// Tooltip logic for flow diagram
const svg = document.getElementById('flow-diagram');
const tooltip = document.getElementById('flow-tooltip');
if (svg && tooltip) {
  svg.addEventListener('mousemove', function(e) {
    const target = e.target;
    if (target.classList.contains('flow-node')) {
      const tip = target.getAttribute('data-tip');
      if (tip) {
        tooltip.textContent = tip;
        tooltip.style.display = 'block';
        // Position tooltip near mouse, but not offscreen
        let x = e.clientX + 18;
        let y = e.clientY - 10;
        if (x + tooltip.offsetWidth > window.innerWidth) x = window.innerWidth - tooltip.offsetWidth - 10;
        if (y + tooltip.offsetHeight > window.innerHeight) y = window.innerHeight - tooltip.offsetHeight - 10;
        tooltip.style.left = x + 'px';
        tooltip.style.top = y + 'px';
      }
    } else {
      tooltip.style.display = 'none';
    }
  });
  svg.addEventListener('mouseleave', function() {
    tooltip.style.display = 'none';
  });
  // Hide tooltip on scroll or click elsewhere
  window.addEventListener('scroll', () => { tooltip.style.display = 'none'; });
  document.body.addEventListener('click', (e) => {
    if (!svg.contains(e.target)) tooltip.style.display = 'none';
  });
}
</script>

<script>
// Add glossary dictionary and tooltip logic
const glossary = {

  "V1": "Primary visual cortex: the first stage of cortical processing for visual input, sensitive to edges and orientations.",
  "V2": "Secondary visual cortex: combines simple features into more complex shapes and patterns.",
  "IT cortex": "Inferotemporal cortex: a region involved in recognizing complex objects and faces.",
  "convolution": "A mathematical operation where a small filter (kernel) slides over an image to extract features like edges or textures.",
  "activation": "A non-linear function applied to a neuron's output, allowing neural networks to learn complex patterns.",
  "pooling": "A process that reduces the spatial size of feature maps, keeping important information while making computation more efficient.",
  "convolutional neural network": "A type of neural network that uses convolutional layers to process data, especially images.",
  "kernel": "A small matrix (filter) used in convolution to extract features from an image.",
  "sobel filter": "A filter used to detect edges in images by highlighting areas of high spatial gradient.",
  "sharpen filter": "A filter that enhances edges and fine details in an image.",
  "blur filter": "A filter that smooths an image by averaging neighboring pixels, reducing detail.",
};

const glossaryTooltip = document.getElementById('glossary-tooltip');
let tooltipTimeout;

document.addEventListener('mouseover', function(e) {
  const term = e.target.closest('.glossary-term');
  if (term) {
    const key = term.getAttribute('data-term');
    if (glossary[key]) {
      glossaryTooltip.textContent = glossary[key];
      glossaryTooltip.classList.add('visible');
      // Position tooltip near mouse
      document.onmousemove = function(ev) {
        let x = ev.clientX + 18;
        let y = ev.clientY - 10;
        if (x + glossaryTooltip.offsetWidth > window.innerWidth) x = window.innerWidth - glossaryTooltip.offsetWidth - 10;
        if (y + glossaryTooltip.offsetHeight > window.innerHeight) y = window.innerHeight - glossaryTooltip.offsetHeight - 10;
        glossaryTooltip.style.left = x + 'px';
        glossaryTooltip.style.top = y + 'px';
      };
    }
  }
});
document.addEventListener('mouseout', function(e) {
  if (e.target.classList.contains('glossary-term')) {
    glossaryTooltip.classList.remove('visible');
    glossaryTooltip.style.left = '-9999px';
    glossaryTooltip.style.top = '-9999px';
    document.onmousemove = null;
  }
});
// Touch support
let lastTouchTerm = null;
document.addEventListener('touchstart', function(e) {
  const term = e.target.closest('.glossary-term');
  if (term) {
    const key = term.getAttribute('data-term');
    if (glossary[key]) {
      glossaryTooltip.textContent = glossary[key];
      glossaryTooltip.classList.add('visible');
      const rect = term.getBoundingClientRect();
      let x = rect.right + 8;
      let y = rect.top;
      if (x + glossaryTooltip.offsetWidth > window.innerWidth) x = window.innerWidth - glossaryTooltip.offsetWidth - 10;
      if (y + glossaryTooltip.offsetHeight > window.innerHeight) y = window.innerHeight - glossaryTooltip.offsetHeight - 10;
      glossaryTooltip.style.left = x + 'px';
      glossaryTooltip.style.top = y + 'px';
      lastTouchTerm = term;
      e.preventDefault();
    }
  } else if (lastTouchTerm) {
    glossaryTooltip.classList.remove('visible');
    lastTouchTerm = null;
  }
}, {passive: false});

// Intersection Observer for fade/slide-in
const fadeSections = document.querySelectorAll('.fade-section');
const observer = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      entry.target.classList.add('visible');
      observer.unobserve(entry.target);
    }
  });
}, { threshold: 0.18 });
fadeSections.forEach(section => observer.observe(section));
</script>
</body>
</html>
